{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_filenames = [f for f in os.listdir('./devset/') if '.tokens' in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции для работы с файлами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, position, length, text):\n",
    "        self._position = position\n",
    "        self._length = length\n",
    "        self._text = text\n",
    "        self._pos = None\n",
    "        self._tag = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Span:\n",
    "    def __init__(self, token_id):\n",
    "        self._token_id = token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(token_filename, dev=True):\n",
    "    _set = './devset/' if dev else './testset/'\n",
    "    tokens = dict()\n",
    "    with open(_set + token_filename, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            if len(split) > 0:\n",
    "                t = Token(split[1], split[2], split[3])\n",
    "                tokens[split[0]] = t\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spans(token_filename, dev=True):\n",
    "    _set = './devset/' if dev else './testset/'\n",
    "    spans = dict()\n",
    "    with open(_set + token_filename.split('.')[0] + '.spans', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            s = Span(split[4])\n",
    "            spans[split[0]] = s\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_base_tag(base_tag):\n",
    "    if base_tag == 'Person':\n",
    "        return 'PER'\n",
    "    if base_tag == 'Location' or base_tag == 'LocOrg':\n",
    "        return 'LOC'\n",
    "    if base_tag == 'Org':\n",
    "        return 'ORG'\n",
    "    else:\n",
    "        return 'MISC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_objects(token_filename, tokens, spans, dev=True):\n",
    "    _set = './devset/' if dev else './testset/'\n",
    "    with open(_set + token_filename.split('.')[0] + '.objects', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.split(' # ')[0]\n",
    "            split = line.split()\n",
    "            base_tag = transform_base_tag(split[1])\n",
    "            span_ids = split[2:]\n",
    "            if len(span_ids) == 1:\n",
    "                tokens[spans[span_ids[0]]._token_id]._tag = 'U-' + base_tag\n",
    "            else:\n",
    "                for i, span_id in enumerate(span_ids):\n",
    "                    if i == 0:\n",
    "                        tokens[spans[span_ids[i]]._token_id]._tag = 'B-' + base_tag\n",
    "                    if i == len(span_ids) - 1:\n",
    "                        tokens[spans[span_ids[i]]._token_id]._tag = 'L-' + base_tag\n",
    "                    else:\n",
    "                        tokens[spans[span_ids[i]]._token_id]._tag = 'I-' + base_tag\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_pos(tokens):\n",
    "    for id, token in tokens.items():\n",
    "        pos = morph.parse(token._text)[0].tag.POS\n",
    "        if pos is None:\n",
    "            pos = 'None'\n",
    "        token._pos = pos\n",
    "        if token._tag is None:\n",
    "            token._tag = 'O'\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i]._text\n",
    "    postag = sent[i]._pos\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]._text\n",
    "        postag1 = sent[i-1]._pos\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]._text\n",
    "        postag1 = sent[i+1]._pos\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [token._tag for token in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_by_sents(tokens):\n",
    "    sents = []\n",
    "    sent = []\n",
    "    for id, token in tokens.items():\n",
    "        if token._text != '.':\n",
    "            sent.append(token)\n",
    "        else:\n",
    "            sents.append(sent)\n",
    "            sent = []\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовим тренировочную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sents(token_filename):\n",
    "    tokens = load_tokens(token_filename)\n",
    "    spans = load_spans(token_filename)\n",
    "    tokens = load_objects(token_filename, tokens, spans)\n",
    "    tokens = fill_pos(tokens)\n",
    "    sents = split_tokens_by_sents(tokens)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb56236c1ef4cb39daa79e6120d0614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "for token_filename in tqdm_notebook(token_filenames):\n",
    "    sents += generate_sents(token_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_ids, test_ids = train_test_split(np.arange(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = np.array(sents)[train_ids]\n",
    "test_sents = np.array(sents)[test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.1, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      I-LOC       0.42      0.24      0.30        21\n",
      "      I-ORG       0.39      0.17      0.24        80\n",
      "      I-PER       0.89      0.96      0.92       112\n",
      "      L-LOC       0.27      0.17      0.21        18\n",
      "      L-ORG       0.41      0.17      0.24        64\n",
      "      L-PER       0.88      0.99      0.93       106\n",
      "          O       0.98      0.99      0.99      6126\n",
      "      U-LOC       0.75      0.88      0.81       179\n",
      "      U-ORG       0.80      0.66      0.72       101\n",
      "      U-PER       0.79      0.45      0.57        76\n",
      "\n",
      "avg / total       0.95      0.96      0.95      6883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "print(flat_classification_report(y_test, crf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применяем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(tokens):\n",
    "    rows = []\n",
    "\n",
    "    buffer = []\n",
    "    for id, token in tokens.items():\n",
    "        tag = token._tag\n",
    "        if tag.startswith('U'):\n",
    "            rows.append('%s %d %d\\n' % (tag.split('-')[1], int(token._position), int(token._length)))\n",
    "        elif tag.startswith('B') or tag.startswith('I'):\n",
    "            buffer.append(token)\n",
    "        elif tag.startswith('L'):\n",
    "            buffer.append(token)\n",
    "            start = int(buffer[0]._position)\n",
    "            length = int(buffer[-1]._position) + int(buffer[-1]._length) - int(start)\n",
    "            rows.append('%s %d %d\\n' % (tag.split('-')[1], start, length))\n",
    "            buffer = []\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_filenames = [filename for filename in os.listdir('./testset') if '.tokens' in filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4cf43f0d744341834369ed3d73744e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=132), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for token_filename in tqdm_notebook(test_token_filenames):\n",
    "    tokens = load_tokens(token_filename, dev=False)\n",
    "    tokens = fill_pos(tokens)\n",
    "    sents = split_tokens_by_sents(tokens)\n",
    "    X = [sent2features(s) for s in sents]\n",
    "    y_pred = crf.predict(X)\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_pred[i])):\n",
    "            sents[i][j]._tag = y_pred[i][j]\n",
    "    rows = get_entities(tokens)\n",
    "    with open('./results_crf/' + token_filename.split('.')[0] + '.task1', 'w') as f:\n",
    "        f.writelines(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверяем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type    P        R        F1       TP1      TP2      In Std.  In Test.\n",
      "per        0.7470   0.6666   0.7045   897.88   897.88     1347     1202\n",
      "loc        0.3239   0.7976   0.4607   477.78   477.78      599     1475\n",
      "org        0.7129   0.3661   0.4837   583.87   583.87     1595      819\n",
      "locorg     1.0000   0.0000   0.0000     0.00     0.00      633        0\n",
      "overall    0.5605   0.4695   0.5110  1959.54  1959.54     4174     3496\n"
     ]
    }
   ],
   "source": [
    "!python scripts\\t1_eval.py -s .\\testset -t .\\results_crf -o .\\output\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
